{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uP5hgXNLaV1u"
   },
   "source": [
    "# MACHINE LEARNING LAB ASSIGNMENT\n",
    "\n",
    "\n",
    "# Neural Network Assignment\n",
    "\n",
    "\n",
    "### NAME     : **MOHIT TALREJA**\n",
    "\n",
    "### ROLL NO. : **177237**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1GFxGWFLO3gc"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCWbT-dmb6vI"
   },
   "source": [
    "### Loading the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fp0XF0WzapFF"
   },
   "outputs": [],
   "source": [
    "features, labels = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D49MHtMuc-0I"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyajgOeZbLUo"
   },
   "source": [
    "Normalizing the feature set:\n",
    "\n",
    "Each entry is a value between 0 and 255. It is brought to a value between 0 and 1 so as to avoid issues with activation function calculations. (Overflowing when taken directly as it is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0XV08rNSbGPZ"
   },
   "outputs": [],
   "source": [
    "features = (features/255).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cxYnu99cJKQ"
   },
   "source": [
    "Converting the labels to 1-hot encoded format so as to allow for simple subtraction in delta calculation step at the outputs of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "By6YqRNHbIUs"
   },
   "outputs": [],
   "source": [
    "labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfQMD-84cfA4"
   },
   "source": [
    "Splitting into training and testing parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PkiflyCCcbTZ"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size=0.15, random_state=37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC9C8HgzczsT"
   },
   "source": [
    "### Neural Network Construction\n",
    "\n",
    "**Number of Neurons in Input Layer** = 28 x 28 = 784\n",
    "\n",
    "**Number of Hidden Layers** = 2\n",
    "\n",
    "**Number of Neurons in Output Layer** = Number of prediction labels = 10\n",
    "\n",
    "**Weighted Sum:**\n",
    "This is the scalar dot product of the 2 vectors, weights and features.\n",
    "\n",
    "$ z = \\sum_{i=0}^{n}w_{i}.x_{i}$\n",
    "\n",
    "**Activation Function for Hidden Layer Units**: \n",
    "The sigmoid activation function takes input the weighted sum and computes the following - \n",
    "\n",
    "$f(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "**Activation Function for Output Layer Units**:\n",
    "The softmax activation function takes input the weighted sum of outputs from hidden layer units and computes the following - \n",
    "\n",
    "$softmax(z) = \\frac{exp(z)}{\\sum_{j}^{ }exp(z_j)}$\n",
    "\n",
    "This softmax gives us the probability of each prediction label.\n",
    "\n",
    "**Forward Propogation**\n",
    "\n",
    "Hidden layer 1 takes input the feature tuple, computes weighted sum, applies sigmoid activation to calculate the output.\n",
    "\n",
    "Hidden layer 2 takes input as the outputs from hidden layer 1, computes weighted sum, applies sigmoid activation to calculate the output.\n",
    "\n",
    "Output layer takes input as the outputs from hidden layer 2, computes weighted sum, applies softmax activation to calculate prediction of each label digit.\n",
    "\n",
    "**Backward Propogation**\n",
    "\n",
    "Calculates the delta (change) values of network parameters:\n",
    "\n",
    "Output layer's delta value is calculated with difference between actual labelled outputs (1-hot encoded vector) and the prediciton values from forward propogation step along with the softmax derivative of it's output.\n",
    "\n",
    "Hidden layer 1's delta value is calculated with weighted sum of delta values of output layer along with the sigmoid derivative of it's output.\n",
    "\n",
    "Hidden layer 2's delta value is calculated with weighted sum of delta values of hidden layer 1 along with the sigmoid derivative of it's output.\n",
    "\n",
    "**Training the Model**\n",
    "\n",
    "Repeat for EPOCH iterations of the dataset:\n",
    "\n",
    "Forward propogate features\n",
    "\n",
    "Backpropogate errors\n",
    "\n",
    "Update weights (stochastic update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "w9DrD8XPP4v6"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    #Initializing the network\n",
    "    def __init__(self, sizes, epochs=50, lRate=0.01):\n",
    "        self.sizes = sizes # Number of neurons in each layer\n",
    "        self.epochs = epochs\n",
    "        self.lRate = lRate # Learning rate\n",
    "        self.parameters = self.initParameters()\n",
    "\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x, derivative=False):\n",
    "        exps = np.exp(x - x.max())\n",
    "        if derivative:\n",
    "            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "    \n",
    "    # Initializing the weights and biases of the network\n",
    "    def initParameters(self):\n",
    "        parameters = {}\n",
    "        for i in range(1,len(self.sizes)):\n",
    "            #parameters['W'+str(i)] = np.full((self.sizes[i],self.sizes[i-1]),0.1)\n",
    "            parameters['W'+str(i)] = np.random.randn(self.sizes[i], self.sizes[i-1]) * np.sqrt(1. / self.sizes[i])\n",
    "\n",
    "        \"\"\"\n",
    "        input_layer=self.sizes[0]\n",
    "        hidden_1=self.sizes[1]\n",
    "        hidden_2=self.sizes[2]\n",
    "        output_layer=self.sizes[3]\n",
    "\n",
    "        # Initializing all weights to values between 0 and 1 \n",
    "        parameters = {\n",
    "            'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
    "            'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
    "            'W3':np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
    "        }\n",
    "        \"\"\"\n",
    "        return parameters\n",
    "\n",
    "    def forwardPropogate(self, X_train):\n",
    "        parameters = self.parameters\n",
    "        # A0 denotes input to hidden layer 1, which is the feature vector\n",
    "        parameters['A0'] = X_train\n",
    "        for i in range(1,len(self.sizes)):\n",
    "            parameters['Z'+str(i)] = np.dot(parameters['W'+str(i)], parameters['A'+str(i-1)])\n",
    "            parameters['A'+str(i)] = self.sigmoid(parameters['Z'+str(i)])\n",
    "        \"\"\"\n",
    "        # Z1 is weighted sum of inputs\n",
    "        parameters['Z1'] = np.dot(parameters[\"W1\"], parameters['A0'])\n",
    "        # A1 denotes input to hidden layer 2, which is the output of hidden layer 1\n",
    "        parameters['A1'] = self.sigmoid(parameters['Z1'])\n",
    "\n",
    "        # Z2 is weighted sum of outputs from hidden layer 1\n",
    "        parameters['Z2'] = np.dot(parameters[\"W2\"], parameters['A1'])\n",
    "        # A2 denotes input to output layer, which is the output of hidden layer 2\n",
    "        parameters['A2'] = self.sigmoid(parameters['Z2'])\n",
    "\n",
    "        # Z3 is weighted sum of outputs from hidden layer 2\n",
    "        parameters['Z3'] = np.dot(parameters[\"W3\"], parameters['A2'])\n",
    "        # A3 denotes final prediction\n",
    "        parameters['A3'] = self.softmax(parameters['Z3'])\n",
    "        \"\"\"\n",
    "        return parameters['A'+str(len(self.sizes)-1)]\n",
    "\n",
    "    def backwardPropogate(self, y_train, output):\n",
    "        \n",
    "        parameters = self.parameters\n",
    "        deltaW = {}\n",
    "\n",
    "        j = len(self.sizes)-1\n",
    "        # Output layer's delta\n",
    "        #delta = 2 * (output - y_train) / output.shape[0] * self.softmax(parameters['Z'+str(j)], derivative=True)\n",
    "        delta = output*([1 for _ in range(len(output))]-output)*(output-y_train)\n",
    "        # Weight change value for output layer\n",
    "        \n",
    "\n",
    "        while(j>0):\n",
    "          # weights and delta have the same shape, so we need to perform transpose on weights to calculate dot product\n",
    "          # Hidden layer 1's delta is weighted sum of delta values of output layer's neurons\n",
    "            deltaW['W'+str(j)] = np.outer(delta, parameters['A'+str(j-1)])\n",
    "            if(j>1):\n",
    "                delta = np.dot(parameters['W'+str(j)].T, delta) * self.sigmoid(parameters['Z'+str(j-1)], derivative=True)\n",
    "            j-=1\n",
    "          \n",
    "        \"\"\"\n",
    "        # Hidden layer 1's delta is weighted sum of delta values of output layer's neurons\n",
    "        delta = np.dot(parameters['W3'].T, delta) * self.sigmoid(parameters['Z2'], derivative=True)\n",
    "        # Weight change value for hidden layer 1\n",
    "        deltaW['W2'] = np.outer(delta, parameters['A1'])\n",
    "\n",
    "        # Hidden layer 2's delta is weighted sum of delta values of hidden layer 1's neurons\n",
    "        delta = np.dot(parameters['W2'].T, delta) * self.sigmoid(parameters['Z1'], derivative=True)\n",
    "        # Weight change value for hidden layer 2\n",
    "        deltaW['W1'] = np.outer(delta, parameters['A0'])\n",
    "        \"\"\"\n",
    "        return deltaW\n",
    "\n",
    "    def updateParameters(self, deltaW):\n",
    "        \n",
    "        for key, value in deltaW.items():\n",
    "            self.parameters[key] -= self.lRate * value\n",
    "\n",
    "    def calculateAccuracy(self, X_test, Y_test):\n",
    "        predictions = []\n",
    "\n",
    "        for x, y in zip(X_test, Y_test):\n",
    "            output = self.forwardPropogate(x)\n",
    "            pred = np.argmax(output)\n",
    "            predictions.append(pred == np.argmax(y))\n",
    "        \n",
    "        return np.mean(predictions)\n",
    "\n",
    "    def train(self, X_train, y_train, X_test, Y_test):\n",
    "        start_time = time.time()\n",
    "        for iteration in range(self.epochs):\n",
    "            for x,y in zip(X_train, y_train):\n",
    "                output = self.forwardPropogate(x)\n",
    "                deltaW = self.backwardPropogate(y, output)\n",
    "                self.updateParameters(deltaW)\n",
    "            \n",
    "            #print(self.parameters)\n",
    "            accuracy = self.calculateAccuracy(X_test, Y_test)\n",
    "            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
    "                iteration+1, time.time() - start_time, accuracy * 100\n",
    "            ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnzcCiTnH7Cv"
   },
   "source": [
    "**Number of neurons:** \n",
    "\n",
    "Input layer = 784\n",
    "\n",
    "Hidden layer 1 = 32\n",
    "\n",
    "Hidden layer 2 = 16\n",
    "\n",
    "Output layer = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_OadvE2ivmBP",
    "outputId": "16234c9e-94d0-4da3-c5c9-16c67cb69171"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Time Spent: 62.05s, Accuracy: 19.06%\n",
      "Epoch: 2, Time Spent: 123.93s, Accuracy: 28.04%\n",
      "Epoch: 3, Time Spent: 185.79s, Accuracy: 33.66%\n",
      "Epoch: 4, Time Spent: 248.10s, Accuracy: 39.74%\n",
      "Epoch: 5, Time Spent: 310.03s, Accuracy: 59.26%\n",
      "Epoch: 6, Time Spent: 371.74s, Accuracy: 65.73%\n",
      "Epoch: 7, Time Spent: 433.65s, Accuracy: 74.04%\n",
      "Epoch: 8, Time Spent: 496.30s, Accuracy: 80.55%\n",
      "Epoch: 9, Time Spent: 557.90s, Accuracy: 84.50%\n",
      "Epoch: 10, Time Spent: 625.49s, Accuracy: 86.64%\n"
     ]
    }
   ],
   "source": [
    "nnObj = NeuralNetwork(sizes=[784, 128, 64, 32, 10],epochs=10)\n",
    "nnObj.train(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKjfstSCIis9"
   },
   "source": [
    "### Loading Handwritten Character Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tSUoFa7L_T3B",
    "outputId": "7c9a21df-c1d8-4f2e-b1b1-a8adcca78e39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0aj540v_pgq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97uOqBD-_wVs",
    "outputId": "356a3e24-02f3-4db5-dbf3-d00d067f8145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/Kaggle\n"
     ]
    }
   ],
   "source": [
    "#changing the working directory\n",
    "%cd /content/gdrive/My Drive/Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vm_4Ku20_4Sl",
    "outputId": "93373e58-389b-41a4-a909-958f5e2b74c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading az-handwritten-alphabets-in-csv-format.zip to /content/gdrive/My Drive/Kaggle\n",
      " 96% 177M/185M [00:07<00:00, 21.5MB/s]\n",
      "100% 185M/185M [00:07<00:00, 24.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d sachinpatel21/az-handwritten-alphabets-in-csv-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xxd9BbKjAE58",
    "outputId": "9176999b-70ad-41ac-baf1-34dd882abb11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "az-handwritten-alphabets-in-csv-format.zip  kaggle.json\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GniQCS9YAHlV",
    "outputId": "329caaa8-f3f7-483c-b4a4-79ac223f36b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  az-handwritten-alphabets-in-csv-format.zip\n",
      "  inflating: A_Z Handwritten Data.csv  \n",
      "  inflating: A_Z Handwritten Data/A_Z Handwritten Data.csv  \n"
     ]
    }
   ],
   "source": [
    "#unzipping the zip files and deleting the zip files\n",
    "!unzip \\*.zip  && rm *.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGTRoqhNARvb",
    "outputId": "c7778eef-f9ca-4d79-9cd6-c5393affadbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'A_Z Handwritten Data'\t'A_Z Handwritten Data.csv'   kaggle.json\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y2Du68vaAaHD",
    "outputId": "7b38c17d-fd3b-47cf-dbac-c7bab7d6521a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(372450, 785)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('A_Z Handwritten Data.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JThYrgF2ItTz"
   },
   "source": [
    "Taking subset of the dataset (first 60000 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkTkFGJlCs2_"
   },
   "outputs": [],
   "source": [
    "df = np.array(df)\n",
    "X = df[:60000,1:785]\n",
    "Y = df[:60000,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ov2jjjxFDH_D",
    "outputId": "6992646d-d48d-433f-f376-a3f25f010810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 1)\n",
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(np.unique(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylu7ZuRgIzq1"
   },
   "source": [
    "Normalizing feature set entries and converting labels to 1-hot encoded form (same as above done for MNIST dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPuy55OwFQHx"
   },
   "outputs": [],
   "source": [
    "X = (X/255).astype('float32')\n",
    "Y = to_categorical(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mt_DnkMlDh6G"
   },
   "outputs": [],
   "source": [
    "CX_train, CX_test, CY_train, CY_test = train_test_split(X, Y, test_size=0.15, random_state=37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRvmbQSNIY3U"
   },
   "source": [
    "**Number of neurons:** \n",
    "\n",
    "Input layer = 784\n",
    "\n",
    "Hidden layer 1 = 64\n",
    "\n",
    "Hidden layer 2 = 32\n",
    "\n",
    "Output layer = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwBLKWjWDwiv",
    "outputId": "a9b8dd75-675b-4bf1-e957-7e4998326134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Time Spent: 30.26s, Accuracy: 88.08%\n",
      "Epoch: 2, Time Spent: 60.59s, Accuracy: 91.08%\n",
      "Epoch: 3, Time Spent: 90.86s, Accuracy: 93.29%\n",
      "Epoch: 4, Time Spent: 121.14s, Accuracy: 94.41%\n",
      "Epoch: 5, Time Spent: 151.34s, Accuracy: 94.78%\n"
     ]
    }
   ],
   "source": [
    "nnObjChar = NeuralNetwork(sizes=[784, 64, 32, 5],epochs=5)\n",
    "nnObjChar.train(CX_train, CY_train, CX_test, CY_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfRkLnEDTasE"
   },
   "source": [
    "### Adaline Multi Layer Network for learning XOR Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NASFITudSy96"
   },
   "outputs": [],
   "source": [
    "X_XOR = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y_XOR = [0,1,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3LShaQ2mTBys",
    "outputId": "585a9fc2-2b7e-4d98-ccc0-b7fa88c69b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 0.19387601, -0.08574695,  0.00645916],\n",
      "       [ 0.07529335, -1.7797719 , -1.73207747]]), 'W2': array([[ 0.65755297,  0.54557789],\n",
      "       [ 0.46994111, -0.5705033 ]]), 'W3': array([[ 0.18529898, -1.14961815]]), 'A0': array([1, 1, 1]), 'Z1': array([ 0.11458822, -3.43655601]), 'A1': array([0.52861575, 0.03117233]), 'Z2': array([0.36459979, 0.23063436]), 'A2': array([0.59015345, 0.55740436]), 'Z3': array([-0.53144734]), 'A3': array([0.37017938])}\n",
      "Epoch: 1, Time Spent: 0.00s, Accuracy: 100.00%\n",
      "{'W1': array([[ 0.19387601, -0.08574695,  0.00645916],\n",
      "       [ 0.07529335, -1.7797719 , -1.73207747]]), 'W2': array([[ 0.65755297,  0.54557789],\n",
      "       [ 0.46994111, -0.5705033 ]]), 'W3': array([[ 0.18529898, -1.14961815]]), 'A0': array([1, 1, 1]), 'Z1': array([ 0.11458822, -3.43655601]), 'A1': array([0.52861575, 0.03117233]), 'Z2': array([0.36459979, 0.23063436]), 'A2': array([0.59015345, 0.55740436]), 'Z3': array([-0.53144734]), 'A3': array([0.37017938])}\n",
      "Epoch: 2, Time Spent: 0.00s, Accuracy: 100.00%\n",
      "{'W1': array([[ 0.19387601, -0.08574695,  0.00645916],\n",
      "       [ 0.07529335, -1.7797719 , -1.73207747]]), 'W2': array([[ 0.65755297,  0.54557789],\n",
      "       [ 0.46994111, -0.5705033 ]]), 'W3': array([[ 0.18529898, -1.14961815]]), 'A0': array([1, 1, 1]), 'Z1': array([ 0.11458822, -3.43655601]), 'A1': array([0.52861575, 0.03117233]), 'Z2': array([0.36459979, 0.23063436]), 'A2': array([0.59015345, 0.55740436]), 'Z3': array([-0.53144734]), 'A3': array([0.37017938])}\n",
      "Epoch: 3, Time Spent: 0.01s, Accuracy: 100.00%\n",
      "{'W1': array([[ 0.19387601, -0.08574695,  0.00645916],\n",
      "       [ 0.07529335, -1.7797719 , -1.73207747]]), 'W2': array([[ 0.65755297,  0.54557789],\n",
      "       [ 0.46994111, -0.5705033 ]]), 'W3': array([[ 0.18529898, -1.14961815]]), 'A0': array([1, 1, 1]), 'Z1': array([ 0.11458822, -3.43655601]), 'A1': array([0.52861575, 0.03117233]), 'Z2': array([0.36459979, 0.23063436]), 'A2': array([0.59015345, 0.55740436]), 'Z3': array([-0.53144734]), 'A3': array([0.37017938])}\n",
      "Epoch: 4, Time Spent: 0.01s, Accuracy: 100.00%\n",
      "{'W1': array([[ 0.19387601, -0.08574695,  0.00645916],\n",
      "       [ 0.07529335, -1.7797719 , -1.73207747]]), 'W2': array([[ 0.65755297,  0.54557789],\n",
      "       [ 0.46994111, -0.5705033 ]]), 'W3': array([[ 0.18529898, -1.14961815]]), 'A0': array([1, 1, 1]), 'Z1': array([ 0.11458822, -3.43655601]), 'A1': array([0.52861575, 0.03117233]), 'Z2': array([0.36459979, 0.23063436]), 'A2': array([0.59015345, 0.55740436]), 'Z3': array([-0.53144734]), 'A3': array([0.37017938])}\n",
      "Epoch: 5, Time Spent: 0.01s, Accuracy: 100.00%\n",
      "{'W1': array([[ 0.19387601, -0.08574695,  0.00645916],\n",
      "       [ 0.07529335, -1.7797719 , -1.73207747]]), 'W2': array([[ 0.65755297,  0.54557789],\n",
      "       [ 0.46994111, -0.5705033 ]]), 'W3': array([[ 0.18529898, -1.14961815]]), 'A0': array([1, 1, 1]), 'Z1': array([ 0.11458822, -3.43655601]), 'A1': array([0.52861575, 0.03117233]), 'Z2': array([0.36459979, 0.23063436]), 'A2': array([0.59015345, 0.55740436]), 'Z3': array([-0.53144734]), 'A3': array([0.37017938])}\n",
      "Epoch: 6, Time Spent: 0.01s, Accuracy: 100.00%\n",
      "{'W1': array([[ 0.19387601, -0.08574695,  0.00645916],\n",
      "       [ 0.07529335, -1.7797719 , -1.73207747]]), 'W2': array([[ 0.65755297,  0.54557789],\n",
      "       [ 0.46994111, -0.5705033 ]]), 'W3': array([[ 0.18529898, -1.14961815]]), 'A0': array([1, 1, 1]), 'Z1': array([ 0.11458822, -3.43655601]), 'A1': array([0.52861575, 0.03117233]), 'Z2': array([0.36459979, 0.23063436]), 'A2': array([0.59015345, 0.55740436]), 'Z3': array([-0.53144734]), 'A3': array([0.37017938])}\n",
      "Epoch: 7, Time Spent: 0.01s, Accuracy: 100.00%\n",
      "{'W1': array([[ 0.19387601, -0.08574695,  0.00645916],\n",
      "       [ 0.07529335, -1.7797719 , -1.73207747]]), 'W2': array([[ 0.65755297,  0.54557789],\n",
      "       [ 0.46994111, -0.5705033 ]]), 'W3': array([[ 0.18529898, -1.14961815]]), 'A0': array([1, 1, 1]), 'Z1': array([ 0.11458822, -3.43655601]), 'A1': array([0.52861575, 0.03117233]), 'Z2': array([0.36459979, 0.23063436]), 'A2': array([0.59015345, 0.55740436]), 'Z3': array([-0.53144734]), 'A3': array([0.37017938])}\n",
      "Epoch: 8, Time Spent: 0.02s, Accuracy: 100.00%\n",
      "{'W1': array([[ 0.19387601, -0.08574695,  0.00645916],\n",
      "       [ 0.07529335, -1.7797719 , -1.73207747]]), 'W2': array([[ 0.65755297,  0.54557789],\n",
      "       [ 0.46994111, -0.5705033 ]]), 'W3': array([[ 0.18529898, -1.14961815]]), 'A0': array([1, 1, 1]), 'Z1': array([ 0.11458822, -3.43655601]), 'A1': array([0.52861575, 0.03117233]), 'Z2': array([0.36459979, 0.23063436]), 'A2': array([0.59015345, 0.55740436]), 'Z3': array([-0.53144734]), 'A3': array([0.37017938])}\n",
      "Epoch: 9, Time Spent: 0.02s, Accuracy: 100.00%\n",
      "{'W1': array([[ 0.19387601, -0.08574695,  0.00645916],\n",
      "       [ 0.07529335, -1.7797719 , -1.73207747]]), 'W2': array([[ 0.65755297,  0.54557789],\n",
      "       [ 0.46994111, -0.5705033 ]]), 'W3': array([[ 0.18529898, -1.14961815]]), 'A0': array([1, 1, 1]), 'Z1': array([ 0.11458822, -3.43655601]), 'A1': array([0.52861575, 0.03117233]), 'Z2': array([0.36459979, 0.23063436]), 'A2': array([0.59015345, 0.55740436]), 'Z3': array([-0.53144734]), 'A3': array([0.37017938])}\n",
      "Epoch: 10, Time Spent: 0.02s, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "nnObjChar = NeuralNetwork(sizes=[2, 2, 2, 1],epochs=5)\n",
    "nnObjChar.train(X_XOR, Y_XOR, X_XOR, Y_XOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NN_Final_Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
